{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SYDE675_Project(2)(1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "import sys    \n",
        "path_to_module = '/content/gdrive/MyDrive/TIP-IM'\n",
        "sys.path.append(path_to_module)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwndgRc7HCJ-",
        "outputId": "1f209145-67e3-4e85-88f7-b3b932b5ad2d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 1.x"
      ],
      "metadata": {
        "id": "suBVGgRZOgXF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d7e2604-8ac2-4ffb-a610-4bd0247a29d5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install numpy==1.16.2\n",
        "!pip3 install scipy==1.2.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rog67q-pVRnD",
        "outputId": "251c0a3a-7560-4d39-f0b5-668df84c57a8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.16.2 in /usr/local/lib/python3.7/dist-packages (1.16.2)\n",
            "Requirement already satisfied: scipy==1.2.1 in /usr/local/lib/python3.7/dist-packages (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from scipy==1.2.1) (1.16.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import os\n",
        "import sys\n",
        "sys.path.append('align_methods')\n",
        "from align import align, re_align\n",
        "from utils import save_priv, cosdistance, inference\n",
        "import math\n",
        "import argparse\n",
        "from scipy.misc import imread\n",
        "from collections import OrderedDict\n",
        "from get_model import getmodel\n",
        "from tqdm import tqdm\n",
        "from input_diversify import input_diversity\n",
        "import cv2\n",
        "from config import threshold\n",
        "from torch.autograd import Variable\n",
        "from mmd import mmd_loss"
      ],
      "metadata": {
        "id": "urhzR6u6FeCK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08fb3e7a-742c-470a-8e1e-607b51020142"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /content/gdrive/MyDrive/TIP-IM/align.py:56: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/MyDrive/TIP-IM/align.py:59: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/MyDrive/TIP-IM/detect_face.py:280: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/MyDrive/TIP-IM/detect_face.py:281: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/MyDrive/TIP-IM/detect_face.py:125: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/MyDrive/TIP-IM/detect_face.py:177: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/MyDrive/TIP-IM/detect_face.py:210: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "WARNING:tensorflow:From /content/gdrive/MyDrive/TIP-IM/detect_face.py:212: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "WARNING:tensorflow:From /content/gdrive/MyDrive/TIP-IM/detect_face.py:213: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "WARNING:tensorflow:From /content/gdrive/MyDrive/TIP-IM/detect_face.py:197: The name tf.nn.xw_plus_b is deprecated. Please use tf.compat.v1.nn.xw_plus_b instead.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#parser.add_argument('--input_file', help='Input directory with images.', type=str, default='data/pairs.txt')\n",
        "\n",
        "#parser.add_argument('--num_iter', help='Number of iterations.', type=int, default=100)\n",
        "\n",
        "#parser.add_argument('--src_model', help='White-box model', type=str, default='ArcFace', choices=threshold.keys())\n",
        "\n",
        "#parser.add_argument('--ensemble_image', help='whether to use multiple images', type=int, default=0, choices=[0, 1])\n",
        "\n",
        "#parser.add_argument('--batch_size', help='batch size', type=int, default=1)\n",
        "\n",
        "#parser.add_argument('--output', help='output directory', type=str, default='output/exppriv')\n",
        "\n",
        "#parser.add_argument('--gain', help='gain function', type=str, default='gain3')\n",
        "\n",
        "#parser.add_argument('--target_nums', help='target_nums', type=int, default=10)\n",
        "\n",
        "#parser.add_argument('--gamma', help='gamma', type=float, default=0.0)\n",
        "\n",
        "#parser.add_argument('--norm', help='select norm', type=str, default='l2')\n",
        "seed = 0"
      ],
      "metadata": {
        "id": "fsvW59shFg7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZguBd_DDuwp"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    model, img_shape = getmodel('ArcFace') # White-box Model\n",
        "    iters = 100\n",
        "    norm = 'l2'\n",
        "    gamma = 0.0\n",
        "\n",
        "    cnt_pairs = 0\n",
        "    candidate_e = [12]\n",
        "    func = {\n",
        "        'gain1': Gain1,\n",
        "        'gain2': Gain2,\n",
        "        'gain3': Gain3\n",
        "        }\n",
        "    \n",
        "    # conduct attacker\n",
        "    attacker_persons = []\n",
        "    with open('/content/gdrive/MyDrive/TIP-IM/data/pairs.txt', 'r') as f:\n",
        "        for pair in f.readlines()[:500]:\n",
        "            attacker_person = pair.strip()\n",
        "            attacker_persons.append(attacker_person)\n",
        "            \n",
        "    random.seed(seed)\n",
        "    cnt = 0\n",
        "    batch_size = 1\n",
        "\n",
        "    # conduct targets, only 10 identities!\n",
        "    target_nums = 10\n",
        "    aligned_target_imgs = []\n",
        "    target_persons = []\n",
        "    with open('/content/gdrive/MyDrive/TIP-IM/targets/target_attacks.txt', 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            target_person = line.strip()\n",
        "            target_persons.append(target_person)\n",
        "\n",
        "    for i in range(target_nums):\n",
        "        target_img = imread(target_persons[i]).astype(np.float32)\n",
        "        align_target_img, _= align(target_img)\n",
        "        align_target_img = cv2.resize(align_target_img, (img_shape[1], img_shape[0]))\n",
        "        aligned_target_imgs.append(align_target_img)\n",
        "    aligned_target_imgs = np.array(aligned_target_imgs)        \n",
        "\n",
        "    # extract features of targets\n",
        "    target_feas = inference(aligned_target_imgs, model, image_shape = img_shape)\n",
        "    target_feas = Variable(target_feas)\n",
        "    \n",
        "    # extract initial features of attackers\n",
        "    n = len(attacker_persons)\n",
        "    m = 1.0\n",
        "    for i in tqdm(range(0, n, batch_size)):\n",
        "        l, r = i, min(i + batch_size, n)\n",
        "        original_images = []\n",
        "        aligned_images = []\n",
        "        aligned_names = []\n",
        "        M = []\n",
        "        for j in range(l, r):\n",
        "            attack_name, id1, id2 = attacker_persons[j].split('\\t')\n",
        "            original_img = imread(\"/content/gdrive/MyDrive/TIP-IM/data/LFW/{}/{}_{:04d}.jpg\".format(attack_name, attack_name, int(id2))).astype(np.float32)\n",
        "            original_images.append(original_img)\n",
        "            align_img, tmpM = align(original_img)\n",
        "            align_img = cv2.resize(align_img, (img_shape[1], img_shape[0]))\n",
        "            aligned_images.append(align_img)\n",
        "            aligned_names.append(\"{}_{:04d}\".format(attack_name, int(id2)))\n",
        "            M.append(tmpM)\n",
        "        aligned_images = np.array(aligned_images)\n",
        "        input_init = torch.Tensor(aligned_images).cuda()\n",
        "        input_init = input_init.permute(0, 3, 1, 2)\n",
        "        init_feas = model.forward(input_init)\n",
        "        init_feas = Variable(init_feas)\n",
        "\n",
        "        # craft protected images \n",
        "        for epsilon in candidate_e:\n",
        "            alpha = 1.5 * epsilon / iters\n",
        "            inputs = torch.Tensor(aligned_images.copy()).cuda()\n",
        "            inputs = inputs.permute(0, 3, 1, 2)\n",
        "            sum_grad = torch.zeros_like(inputs)\n",
        "            min_img = torch.clamp(inputs - epsilon, min=0)\n",
        "            max_img = torch.clamp(inputs + epsilon, max=255)\n",
        "            adv_images = inputs.detach().clone().requires_grad_(True)\n",
        "            for iii in range(iters):\n",
        "                std_proj = random.uniform(0.01, 0.1)\n",
        "                std_rotate = random.uniform(0.01, 0.1)\n",
        "                tmp_advs = []\n",
        "                tmp_grads = []\n",
        "                tmp_losses = []\n",
        "                model.zero_grad()\n",
        "                # introduce input diversity for generalizaion\n",
        "                images = input_diversity(adv_images, std_proj, std_rotate)\n",
        "                adv_feas = model.forward(images)\n",
        "\n",
        "                # compute image-level natural loss by MMD. \n",
        "                loss_mmd = mmd_loss(adv_images.clone().reshape(batch_size,-1), \n",
        "                        inputs.clone().reshape(batch_size,-1))\n",
        "                \n",
        "                # search optimal target \n",
        "                for idx in range(target_nums):\n",
        "                    model.zero_grad()\n",
        "                    loss_i = torch.mean((adv_feas - init_feas) ** 2)\n",
        "                    loss_t = torch.mean((adv_feas - target_feas[idx]) ** 2)\n",
        "\n",
        "                    # total loss \n",
        "                    '''\n",
        "                    loss_mmd does not affect search direction of targets set, just for updating images.\n",
        "                    '''\n",
        "                    loss = loss_t - loss_i + gamma * loss_mmd\n",
        "   \n",
        "                    loss.backward(retain_graph = True)\n",
        "                    grad = adv_images.grad.data.clone()\n",
        "                    grad = grad / grad.abs().mean(dim=[1, 2, 3], keepdim=True)\n",
        "                    \n",
        "                    tmp_sum_grad = m * sum_grad.clone() + grad\n",
        "                    adv_images.grad.data.zero_()\n",
        "                    tmp_adv_images = adv_images.data.clone()\n",
        "                    # infty norm\n",
        "                    if norm == 'linf':\n",
        "                        tmp_adv_images = tmp_adv_images - torch.sign(tmp_sum_grad) * alpha\n",
        "                    elif norm == 'l2':\n",
        "                        factor = np.sqrt(np.prod(img_shape)* 3)\n",
        "                        grad2d = tmp_sum_grad.reshape((tmp_sum_grad.size(0), -1))\n",
        "                        gradnorm = grad2d.norm(p=2, dim=1, keepdim=True)\n",
        "                        grad_unit = grad2d / gradnorm\n",
        "                        delta = -torch.reshape(grad_unit, tmp_sum_grad.size()) * alpha * factor\n",
        "                        tmp_adv_images = tmp_adv_images + delta\n",
        "                    \n",
        "                    tmp_adv_images = torch.max(tmp_adv_images, min_img)\n",
        "                    tmp_adv_images = torch.min(tmp_adv_images, max_img)\n",
        "                    tmp_grads.append(tmp_sum_grad)\n",
        "                    tmp_advs.append(tmp_adv_images)\n",
        "                    tmp_losses.append(loss.data.unsqueeze(0))\n",
        "               \n",
        "                tmp_losses = torch.cat(tmp_losses)\n",
        "                # find best index\n",
        "                best_idx = submodular(target_feas, init_feas, tmp_advs, model, func['gain3'])\n",
        "                sum_grad = tmp_grads[best_idx]\n",
        "                \n",
        "                adv_images = tmp_advs[best_idx]\n",
        "                #print(tmp_losses[best_idx], best_idx)\n",
        "                adv_images = adv_images.detach().requires_grad_(True)\n",
        "            adv_images = adv_images.detach().permute(0, 2, 3, 1).cpu().numpy()\n",
        "                \n",
        "            for j in range(batch_size):\n",
        "                attacker_name = attacker_persons[l + j].split('\\t')[0]\n",
        "                _ = save_priv(adv_images[j], aligned_images[j], original_images[j], attacker_name, M[j], epsilon, 'ArcFace', 'output/exppriv')\n",
        "\n",
        "def submodular(target_feas, init_feas, tmp_advs, model, Gain):\n",
        "    tmp_advs = torch.cat(tmp_advs)\n",
        "    \n",
        "    gains = torch.zeros(size = (len(target_feas),), dtype = torch.float32)\n",
        "    tmpadv_feas = model.forward(tmp_advs)\n",
        "    for idx in range(len(tmpadv_feas)):\n",
        "        gains[idx] = Gain(tmpadv_feas[idx].unsqueeze(0), init_feas, target_feas, idx)\n",
        "    best_idx = torch.argmax(gains)\n",
        "    return best_idx\n",
        "\n",
        "def L2distance(x, y):\n",
        "\treturn torch.sqrt(torch.sum((x - y)**2, dim = 1))\n",
        "\n",
        "def Gain1(adv_fea, init_feas, target_feas, idx=0):\n",
        "    d1 = L2distance(adv_fea, init_feas)\n",
        "    d2 = torch.sum(torch.exp(d1 - L2distance(adv_fea, target_feas)))\n",
        "    return torch.log(1.0 + d2)\n",
        "\n",
        "def Gain2(adv_fea, init_feas, target_feas, idx=0):\n",
        "    d1 = L2distance(adv_fea, init_feas)\n",
        "    d2 = torch.min(torch.exp(d1 - L2distance(adv_fea, target_feas)))\n",
        "    return torch.log(1.0 + d2)\n",
        "\n",
        "def Gain3(adv_fea, init_feas, target_feas, idx=0):\n",
        "    d1 = L2distance(adv_fea, init_feas)\n",
        "    d2 = torch.max(torch.exp(d1 - L2distance(adv_fea, target_feas)))\n",
        "    return torch.log(1.0 + d2)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Linear, Conv2d, BatchNorm1d, BatchNorm2d, PReLU, ReLU, Sigmoid, Dropout, MaxPool2d, \\\n",
        "    AdaptiveAvgPool2d, Sequential, Module\n",
        "from collections import namedtuple\n",
        "from FaceModel import FaceModel\n",
        "\n",
        "\n",
        "# Support: ['IR_50', 'IR_101', 'IR_152', 'IR_SE_50', 'IR_SE_101', 'IR_SE_152']\n",
        "\n",
        "\n",
        "class Flatten(Module):\n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), -1)\n",
        "\n",
        "\n",
        "def l2_norm(input, axis=1):\n",
        "    norm = torch.norm(input, 2, axis, True)\n",
        "    output = torch.div(input, norm)\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "class SEModule(Module):\n",
        "    def __init__(self, channels, reduction):\n",
        "        super(SEModule, self).__init__()\n",
        "        self.avg_pool = AdaptiveAvgPool2d(1)\n",
        "        self.fc1 = Conv2d(\n",
        "            channels, channels // reduction, kernel_size=1, padding=0, bias=False)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.fc1.weight.data)\n",
        "\n",
        "        self.relu = ReLU(inplace=True)\n",
        "        self.fc2 = Conv2d(\n",
        "            channels // reduction, channels, kernel_size=1, padding=0, bias=False)\n",
        "\n",
        "        self.sigmoid = Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        module_input = x\n",
        "        x = self.avg_pool(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.sigmoid(x)\n",
        "\n",
        "        return module_input * x\n",
        "\n",
        "\n",
        "class bottleneck_IR(Module):\n",
        "    def __init__(self, in_channel, depth, stride):\n",
        "        super(bottleneck_IR, self).__init__()\n",
        "        if in_channel == depth:\n",
        "            self.shortcut_layer = MaxPool2d(1, stride)\n",
        "        else:\n",
        "            self.shortcut_layer = Sequential(\n",
        "                Conv2d(in_channel, depth, (1, 1), stride, bias=False), BatchNorm2d(depth))\n",
        "        self.res_layer = Sequential(\n",
        "            BatchNorm2d(in_channel),\n",
        "            Conv2d(in_channel, depth, (3, 3), (1, 1), 1, bias=False), PReLU(depth),\n",
        "            Conv2d(depth, depth, (3, 3), stride, 1, bias=False), BatchNorm2d(depth))\n",
        "\n",
        "    def forward(self, x):\n",
        "        shortcut = self.shortcut_layer(x)\n",
        "        res = self.res_layer(x)\n",
        "\n",
        "        return res + shortcut\n",
        "\n",
        "\n",
        "class bottleneck_IR_SE(Module):\n",
        "    def __init__(self, in_channel, depth, stride):\n",
        "        super(bottleneck_IR_SE, self).__init__()\n",
        "        if in_channel == depth:\n",
        "            self.shortcut_layer = MaxPool2d(1, stride)\n",
        "        else:\n",
        "            self.shortcut_layer = Sequential(\n",
        "                Conv2d(in_channel, depth, (1, 1), stride, bias=False),\n",
        "                BatchNorm2d(depth))\n",
        "        self.res_layer = Sequential(\n",
        "            BatchNorm2d(in_channel),\n",
        "            Conv2d(in_channel, depth, (3, 3), (1, 1), 1, bias=False),\n",
        "            PReLU(depth),\n",
        "            Conv2d(depth, depth, (3, 3), stride, 1, bias=False),\n",
        "            BatchNorm2d(depth),\n",
        "            SEModule(depth, 16)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        shortcut = self.shortcut_layer(x)\n",
        "        res = self.res_layer(x)\n",
        "\n",
        "        return res + shortcut\n",
        "\n",
        "\n",
        "class Bottleneck(namedtuple('Block', ['in_channel', 'depth', 'stride'])):\n",
        "    '''A named tuple describing a ResNet block.'''\n",
        "\n",
        "\n",
        "def get_block(in_channel, depth, num_units, stride=2):\n",
        "\n",
        "    return [Bottleneck(in_channel, depth, stride)] + [Bottleneck(depth, depth, 1) for i in range(num_units - 1)]\n",
        "\n",
        "\n",
        "def get_blocks(num_layers):\n",
        "    if num_layers == 50:\n",
        "        blocks = [\n",
        "            get_block(in_channel=64, depth=64, num_units=3),\n",
        "            get_block(in_channel=64, depth=128, num_units=4),\n",
        "            get_block(in_channel=128, depth=256, num_units=14),\n",
        "            get_block(in_channel=256, depth=512, num_units=3)\n",
        "        ]\n",
        "    elif num_layers == 100:\n",
        "        blocks = [\n",
        "            get_block(in_channel=64, depth=64, num_units=3),\n",
        "            get_block(in_channel=64, depth=128, num_units=13),\n",
        "            get_block(in_channel=128, depth=256, num_units=30),\n",
        "            get_block(in_channel=256, depth=512, num_units=3)\n",
        "        ]\n",
        "    elif num_layers == 152:\n",
        "        blocks = [\n",
        "            get_block(in_channel=64, depth=64, num_units=3),\n",
        "            get_block(in_channel=64, depth=128, num_units=8),\n",
        "            get_block(in_channel=128, depth=256, num_units=36),\n",
        "            get_block(in_channel=256, depth=512, num_units=3)\n",
        "        ]\n",
        "\n",
        "    return blocks\n",
        "\n",
        "\n",
        "class Backbone(Module):\n",
        "    def __init__(self, input_size, num_layers, mode='ir'):\n",
        "        super(Backbone, self).__init__()\n",
        "        assert input_size[0] in [112, 224], \"input_size should be [112, 112] or [224, 224]\"\n",
        "        assert num_layers in [50, 100, 152], \"num_layers should be 50, 100 or 152\"\n",
        "        assert mode in ['ir', 'ir_se'], \"mode should be ir or ir_se\"\n",
        "        blocks = get_blocks(num_layers)\n",
        "        if mode == 'ir':\n",
        "            unit_module = bottleneck_IR\n",
        "        elif mode == 'ir_se':\n",
        "            unit_module = bottleneck_IR_SE\n",
        "        self.input_layer = Sequential(Conv2d(3, 64, (3, 3), 1, 1, bias=False),\n",
        "                                      BatchNorm2d(64),\n",
        "                                      PReLU(64))\n",
        "        if input_size[0] == 112:\n",
        "            self.output_layer = Sequential(BatchNorm2d(512),\n",
        "                                           Dropout(),\n",
        "                                           Flatten(),\n",
        "                                           Linear(512 * 7 * 7, 512),\n",
        "                                           BatchNorm1d(512))\n",
        "        else:\n",
        "            self.output_layer = Sequential(BatchNorm2d(512),\n",
        "                                           Dropout(),\n",
        "                                           Flatten(),\n",
        "                                           Linear(512 * 14 * 14, 512),\n",
        "                                           BatchNorm1d(512))\n",
        "\n",
        "        modules = []\n",
        "        for block in blocks:\n",
        "            for bottleneck in block:\n",
        "                modules.append(\n",
        "                    unit_module(bottleneck.in_channel,\n",
        "                                bottleneck.depth,\n",
        "                                bottleneck.stride))\n",
        "        self.body = Sequential(*modules)\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = (x - 127.5) / 128\n",
        "        x = self.input_layer(x)\n",
        "        x = self.body(x)\n",
        "        x = self.output_layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.xavier_uniform_(m.weight.data)\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm1d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight.data)\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "\n",
        "\n",
        "def IR_50(input_size):\n",
        "    \"\"\"Constructs a ir-50 model.\n",
        "    \"\"\"\n",
        "    model = Backbone(input_size, 50, 'ir_se')\n",
        "\n",
        "    return model\n",
        "\n",
        "class ArcFace(FaceModel):\n",
        "    def __init__(self, **kwargs):\n",
        "        net = IR_50((112, 112))\n",
        "        url = 'http://ml.cs.tsinghua.edu.cn/~xiaoyang/face_models/ArcFace/model_ir_se50.pth'\n",
        "        channel = 'rgb'\n",
        "        FaceModel.__init__(\n",
        "            self,\n",
        "            net=net,\n",
        "            url=url,\n",
        "            channel=channel,\n",
        "            **kwargs)\n",
        "\n"
      ],
      "metadata": {
        "id": "onHC7flFDiE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from six.moves import urllib\n",
        "url = 'http://ml.cs.tsinghua.edu.cn/~xiaoyang/face_models/ArcFace/model_ir_se50.pth'\n",
        "model_name = url.split('/')[-1]\n",
        "net = IR_50((112, 112))\n",
        "\n",
        "if not os.path.exists('./ckpts/'):\n",
        "\ttry:\n",
        "\t\tos.makedirs('./ckpts/')\n",
        "\texcept OSError as e:\n",
        "\t\tif e.errno != errno.EEXIST:\n",
        "\t\t\traise\n",
        "urllib.request.urlretrieve(url,'./ckpts/{}'.format(model_name))\n",
        "print('Finish downloading')\n",
        "print('Load checkpoint')\n",
        "checkpoint = torch.load('./ckpts/{}'.format(model_name), map_location=lambda storage, loc: storage.cuda())\n",
        "\n",
        "if isinstance(checkpoint, dict) and 'state_dict' in checkpoint:\n",
        "\tnet.load_state_dict(checkpoint['state_dict'])\n",
        "else:\n",
        "\tnet.load_state_dict(checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6RWezCVAWtF",
        "outputId": "6a92c52b-709d-43f3-84e6-706ad02c88fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finish downloading\n",
            "Load checkpoint\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##get name list"
      ],
      "metadata": {
        "id": "VXda9nvuMzzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attacker_persons = []\n",
        "with open('/content/gdrive/MyDrive/TIP-IM/pairs.txt', 'r') as f:\n",
        "    for pair in f.readlines()[:87]:\n",
        "        attacker_person = pair.strip()\n",
        "        attacker_persons.append(attacker_person)\n",
        "\n",
        "\n",
        "# attack_name, id1, id2 = attacker_persons[j].split('\\t')\n",
        "# original_img = imread(\"/content/gdrive/MyDrive/TIP-IM/data/LFW/{}/{}_{:04d}.jpg\".format(attack_name, attack_name, int(id2))).astype(np.float32)\n",
        "model, img_shape = getmodel('ArcFace') # White-box Model\n",
        "batch_size = 1\n",
        "n = len(attacker_persons)\n",
        "m = 1.0\n",
        "names = []\n",
        "for i in tqdm(range(0, n, batch_size)):\n",
        "    l, r = i, min(i + batch_size, n)\n",
        "    \n",
        "    for j in range(l, r):\n",
        "        attack_name, id1, id2 = attacker_persons[j].split('\\t')\n",
        "        names.append(attack_name)\n",
        "        \n",
        "        \n",
        "    \n",
        "names = set(names)\n",
        "names = list(names)\n",
        "print(len(names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "We1be5yFl70V",
        "outputId": "4ae7f20e-5d60-4d33-fdc8-93f6e1e5f707"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load existing checkpoint\n",
            "No existing checkpoint, now downloading online\n",
            "Finish downloading\n",
            "Load checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 87/87 [00:00<00:00, 423323.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##gallery set"
      ],
      "metadata": {
        "id": "ayF2XdhPM6ZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gallery_set = []\n",
        "gallery = os.listdir('/content/gdrive/MyDrive/TIP-IM/gallery')\n",
        "for image in os.listdir('/content/gdrive/MyDrive/TIP-IM/gallery'):\n",
        "  gallery_img = imread('/content/gdrive/MyDrive/TIP-IM/gallery/'+image).astype(np.float32)\n",
        "  align_target_img, _= align(gallery_img)\n",
        "  align_target_img = cv2.resize(align_target_img, (img_shape[1], img_shape[0]))\n",
        "  gallery_feas = inference(align_target_img, model, image_shape = img_shape)\n",
        "  gallery_feas = Variable(gallery_feas)\n",
        "  gallery_feas = gallery_feas.cpu().data\n",
        "  gallery_feas = np.array(gallery_feas)\n",
        "  gallery_set.append(gallery_feas)\n",
        "gallery_set = np.array(gallery_set).reshape(233,-1)\n",
        "print(gallery_set.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoEa-lJpM-Zi",
        "outputId": "71d6a6ef-fe0a-4c61-8126-a0110e4ba6d3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "  after removing the cwd from sys.path.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(233, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Protected image"
      ],
      "metadata": {
        "id": "Xplg9VpLQTY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "protected_set = []\n",
        "for name in names:\n",
        "  protected_img = imread('/content/gdrive/MyDrive/TIP-IM/exppriv/'+name+'-12.png').astype(np.float32)\n",
        "  protected_feas = inference(protected_img, model, image_shape = img_shape)\n",
        "  protected_feas = Variable(protected_feas)\n",
        "  protected_feas = protected_feas.cpu().data\n",
        "  protected_feas = np.array(protected_feas)\n",
        "  protected_set.append(protected_feas)\n",
        "# protected_set = np.array(protected_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5iYaC7NQsla",
        "outputId": "fbfc2047-83b8-41c4-b37c-f8a5de7967f6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##compare"
      ],
      "metadata": {
        "id": "G0pOvzkJSk7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top5 = []\n",
        "for pro_img in protected_set:\n",
        "  distance = []\n",
        "  min = []\n",
        "  for gal_img in gallery_set:\n",
        "    dist = np.linalg.norm(pro_img - gal_img)\n",
        "    distance.append(dist)\n",
        "\n",
        "  for i in range(5):\n",
        "    \n",
        "    ind = np.argmin(distance)\n",
        "    min.append(ind)\n",
        "    distance[ind] = 9999\n",
        "  top5.append(np.array(min))\n",
        "print(top5)"
      ],
      "metadata": {
        "id": "pS3JDt-aSm3z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91fcde0e-4751-4e0d-8e90-068aab1f65fa"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([  7,  51, 131, 198,  50]), array([210, 120, 115, 173,  98]), array([169,  64,  95,  16,  34]), array([ 16, 202,  34, 169, 203]), array([212,  25, 170, 108, 102]), array([220, 128, 211,  42,  25]), array([ 16, 202, 182, 228,  23]), array([212,  25,  96, 102, 109]), array([ 16, 202, 132,  22,  34]), array([216, 215, 147,  37, 156]), array([  7, 131,  51, 178, 181]), array([220, 128, 166, 111, 108]), array([169,  64,  46,  86,  34]), array([212,  25,  28, 108, 131]), array([  7,  51, 131, 198,   5]), array([212,  25, 102,  80, 227]), array([212,  25, 220,  24,   2]), array([ 49,  58, 115, 121,  98]), array([ 16, 202, 169,  71,  42]), array([ 31, 208, 120, 185,  54]), array([ 49,  58, 191, 190,  45]), array([216, 215,  37, 152, 158]), array([ 16, 202,  34, 198,  51]), array([212,  25, 227, 192,  35]), array([ 16, 202, 228, 167,  23]), array([169,  64, 159, 179, 177]), array([182, 103,  16, 130,  47]), array([49, 58, 82, 81, 39]), array([220, 128,   2,  12, 111]), array([ 49,  58,  50, 225,  82]), array([ 49,  58, 206, 225,  50]), array([169,  64,  34,  16,  49]), array([192,  35, 194,  60, 205]), array([ 31,  98,  55, 115, 173]), array([192,  60, 194,  35, 205]), array([169,  64,  57, 159,  16]), array([220, 128, 176,   3, 211]), array([212,  25, 170, 227,  60]), array([ 16, 202, 182,  63, 103]), array([169,  64,  34,  57,  26]), array([212,  25, 226,   0, 170]), array([216, 215,  93, 162, 191]), array([  7,  51, 131,   5, 198]), array([212,  25, 109,   6, 144]), array([  7,  51, 131, 198,   5]), array([  7,  51, 131, 198, 127]), array([216, 215, 101,  37, 161]), array([ 16, 202, 169,  64,  34]), array([  7, 131,  51, 198,   5]), array([ 49,  58,  82, 195, 225])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#taeget set\n",
        "target_nums = 10\n",
        "aligned_target_imgs = []\n",
        "target_persons = []\n",
        "target_names = []\n",
        "with open('/content/gdrive/MyDrive/TIP-IM/target_attacks.txt', 'r') as f:\n",
        "    for line in f.readlines():\n",
        "        target_person = line.strip()\n",
        "        # name, id1, id2 = target_person.split('\\t')\n",
        "        # target_names.append(name)\n",
        "        target_persons.append(target_person)\n",
        "        temp = target_person.split('/')\n",
        "        target_names.append(temp[-2])\n",
        "\n",
        "for i in range(target_nums):\n",
        "    target_img = imread(target_persons[i]).astype(np.float32)\n",
        "    align_target_img, _= align(target_img)\n",
        "    align_target_img = cv2.resize(align_target_img, (img_shape[1], img_shape[0]))\n",
        "    aligned_target_imgs.append(align_target_img)\n",
        "aligned_target_imgs = np.array(aligned_target_imgs)        \n",
        "\n",
        "# extract features of targets\n",
        "target_feas = inference(aligned_target_imgs, model, image_shape = img_shape)\n",
        "target_feas = Variable(target_feas)\n",
        "\n",
        "#name of top5\n",
        "top5_name = []\n",
        "for i in top5:\n",
        "  name_list = []\n",
        "  for j in i:\n",
        "    img = gallery[j]\n",
        "    people_name, num = img.rsplit('_',1)\n",
        "    name_list.append(people_name)\n",
        "  top5_name.append(name_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIe-PLUxsYta",
        "outputId": "289fc87a-cf4a-4cfd-bf4d-a78415b5f5da"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "  app.launch_new_instance()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Rank-5-T"
      ],
      "metadata": {
        "id": "6NueJWN7sOVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#rank-n-t\n",
        "count = 0\n",
        "for i in top5_name:\n",
        "  for j in i:\n",
        "    if j in target_names:\n",
        "      count = count + 1\n",
        "      break\n",
        "print(count/50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7V2XLwfZLb-z",
        "outputId": "5f6abbf7-0b74-47d1-d638-274630d2cdce"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Rank-5-UT"
      ],
      "metadata": {
        "id": "iklGKxB7HvmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for i in range(50):\n",
        "  if names[i] not in top5_name[i]:\n",
        "    count = count + 1\n",
        "print(count/50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHXn5o2PHz6S",
        "outputId": "cecbc950-2c65-4d91-d834-9c61c580563b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Rank-1-T"
      ],
      "metadata": {
        "id": "AeTvtFnkLhja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for i in top5_name:\n",
        "  if i[0] in target_names:\n",
        "    count = count + 1\n",
        "print(count/50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCOL3nTsLkpc",
        "outputId": "e29d739f-09c0-49cf-a9b7-d14f9b8e60c4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Rank-1-UT"
      ],
      "metadata": {
        "id": "qC152RWWMBVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for i in range(50):\n",
        "  if names[i] != top5_name[i][0]:\n",
        "    count = count + 1\n",
        "print(count/50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEh82mKDMAbG",
        "outputId": "e1baae06-33f4-417f-d38a-f2547a753e08"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.98\n"
          ]
        }
      ]
    }
  ]
}